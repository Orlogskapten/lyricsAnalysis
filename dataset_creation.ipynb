{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche de lyrics par artiste\n",
    "\n",
    "Notre but est de récupérer par artiste, l'ensemble des paroles de ses sons.\n",
    "\n",
    "\n",
    "__ATTENTION__ ici je ne me concentre que sur les albums des artistes, c'est à dire que tous les song qui ne sont pas sorties dans un album ne sont pas comtabilisé. En effet, sans cette condition difficile de mettre une année sur la diffusion du son.\n",
    "\n",
    "\n",
    "Nous allons utiliser ce site [https://search.azlyrics.com](https://search.azlyrics.com) pour la recherche des paroles.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Voici donc les étapes de la constitution de cette base:\n",
    "\n",
    "-  Dans un premier temps, la fonction <a href='#artist_to_url'>artist_to_url</a> nous permet de générer à l'aide d'un nom d'artiste un url vers l'ensemble de sa discographie.\n",
    "-  Notre deuxième fonction <a href='#album_url_year'>album_url_year</a> va nous permettre de ressortir JE NE SAIS PAS ENCORE SOUS QUEL FORMAT le nom de l'artiste, le nom de l'album, l'année de difusion et enfin les liens vers les musiques contenus dans les albums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:46:27.786381Z",
     "start_time": "2020-01-11T15:46:21.303054Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from lxml import html\n",
    "import sys\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# import spacy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:46:28.247327Z",
     "start_time": "2020-01-11T15:46:27.789373Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:46:28.261252Z",
     "start_time": "2020-01-11T15:46:28.250285Z"
    }
   },
   "outputs": [],
   "source": [
    "#permet de générer l'url pour accéder à l'ensemble des titres de l'artiste\n",
    "#certain artiste comme Renaud ne sont pas référencé\n",
    "def artist_to_url(artist):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    url_base= \"\"\"https://www.azlyrics.com\"\"\"\n",
    "    name= re.sub(' ','',artist)\n",
    "    name= unidecode.unidecode(name.lower()) #me permet d'enlever les accents comme é ou â\n",
    "    first_l= list(name)[0]\n",
    "    #certains artistes ont un nom qui commence par un chiffre\n",
    "    #dans ce cas, la procédure de création de l'url est un peu différent\n",
    "    if first_l.isalpha():\n",
    "        url= \"\"\"{}/{}/{}.html\"\"\".format(url_base, first_l, name)\n",
    "    else:\n",
    "        url= \"\"\"{}/19/{}.html\"\"\".format(url_base, name)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:46:28.272230Z",
     "start_time": "2020-01-11T15:46:28.264245Z"
    }
   },
   "outputs": [],
   "source": [
    "#test\n",
    "artist_to_url(\"Fewer 333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:46:28.291172Z",
     "start_time": "2020-01-11T15:46:28.274216Z"
    }
   },
   "outputs": [],
   "source": [
    "def album_url_year(artist, timed= True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    year, album_name, url_song= [], [], []\n",
    "    \n",
    "    page= requests.get(artist_to_url(artist)) #l'appelle la fonction précédente\n",
    "    soup= BeautifulSoup(page.text, 'html.parser')\n",
    "    html_page= soup.find('div', id= 'listAlbum')\n",
    "    \n",
    "    if timed:\n",
    "        time.sleep(np.random.randint(5, 30, 1))\n",
    "    \n",
    "    html_data= str(html_page).split('<div class=\"album\"')[1:][:-1]\n",
    "    for ht in html_data:\n",
    "    #je défini mes patterns REGEX pour récupérer de la page html le nom des albums, avec son année et les url des lyrics menant aux paroles de chacunes des musiques de l'album     \n",
    "        pattern_yr= re.compile('</b> \\((\\d{4})\\)</div>|</b> \\((\\d{4})\\)<br/>')\n",
    "        pattern_alb= re.compile('<b>\"([\\s\\S]*)\"</b>')\n",
    "        pattern_url= re.compile('href=\"..([\\s\\S]*).html\" target=')\n",
    "\n",
    "        year.append(pattern_yr.search(ht).group(1))\n",
    "        album_name.append(pattern_alb.search(ht).group(1))\n",
    "        #le traitement pour les url est un peu différent\n",
    "        url_list= ht.split('\\n')\n",
    "        url_base= \"https://www.azlyrics.com\"\n",
    "        url_song.append([url_base + pattern_url.search(href).group(1) + \".html\" \\\n",
    "                for href in url_list \\\n",
    "                if pattern_url.search(href) != None])\n",
    "    try:\n",
    "        df= pd.DataFrame(\n",
    "        {\n",
    "            \"Artiste\": [artist]*len(year),\n",
    "            \"Annee\": year,\n",
    "            \"Album\": album_name,\n",
    "            \"Url\": url_song\n",
    "        })\n",
    "    except ValueError as err:\n",
    "        print(\"Attention, il semble que les listes du df ne soient \\\n",
    "        pas de la même longueur\", err)\n",
    "        \n",
    "    #le df que je creer contient en URF une liste des url, je veux donc avoir une ligne par URL et donc\n",
    "    #les valeurs annee album, et artiste qui correspondent.\n",
    "    df_not_listed= pd.DataFrame({\n",
    "      col:np.repeat(df[col].values, df[\"Url\"].str.len())\n",
    "      for col in df.columns.drop(\"Url\")\n",
    "    }\n",
    "    ).assign(**{\n",
    "        \"Url\":np.concatenate(df[\"Url\"].values)\n",
    "    })[df.columns]\n",
    "    \n",
    "    return df_not_listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T08:41:38.703514Z",
     "start_time": "2020-01-11T08:41:30.233148Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "album_url_year(\"jinjer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T07:41:32.880618Z",
     "start_time": "2020-01-10T07:41:26.696596Z"
    }
   },
   "outputs": [],
   "source": [
    "# artiste_name_style= {\"Slipknot\":\"metal\"\n",
    "# ,\"I Prevail\":\"metal\"\n",
    "# ,\"DREAMERS\":\"Rock/Alternatif\"\n",
    "# ,\"Bring Me The Horizon\":\"metal\"\n",
    "# ,\"DIAMANTE\":\"metal\"\n",
    "# ,\"Motionless in White\":\"Rock\\Alternatif\"\n",
    "# ,\"Demon Hunter\":\"metal\"\n",
    "# ,\"Falling In Reverse\":\"metal\"\n",
    "# }\n",
    "artiste_name_style= {\"Slipknot\":\"metal\"\n",
    ",\"DREAMERS\":\"Rock/Alternatif\"\n",
    ",\"Bring Me The Horizon\":\"metal\"\n",
    ",\"Motionless in White\":\"Rock\\Alternatif\"\n",
    ",\"Falling In Reverse\":\"metal\"\n",
    ",\"Eminem\":\"rap\"\n",
    ",\"Justin Bieber\":\"pop\"\n",
    ",\"Post Malone\":\"rap\"\n",
    ",\"Billie Eilish\":\"pop\"\n",
    ", \"Mac Miller\":\"rap\"\n",
    "}\n",
    "artiste_name= [name for name in artiste_name_style.keys()]\n",
    "\n",
    "df= pd.DataFrame()\n",
    "for art in artiste_name:\n",
    "    print(art)\n",
    "    df= pd.concat([album_url_year(art, False), df], ignore_index= True)\n",
    "\n",
    "print(\"\\nFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T07:41:32.907541Z",
     "start_time": "2020-01-10T07:41:32.884894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ajout colonne Style à partir du dictionnaire\n",
    "df[\"Style\"]= df[\"Artiste\"].map(artiste_name_style)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:47:43.886513Z",
     "start_time": "2020-01-11T15:47:43.878561Z"
    }
   },
   "outputs": [],
   "source": [
    "def verification(df):\n",
    "    print(\"Vérifier si des valeurs sont None\")\n",
    "    for col in df.columns:\n",
    "        nul= (df[col].isnull()).sum()\n",
    "        sh= df.shape[0]\n",
    "        print(\"\\n\")\n",
    "        print(col)\n",
    "        print(nul)\n",
    "        print(\"Soit: {}% de valeurs None\".format((nul/sh)*100))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T07:42:14.992242Z",
     "start_time": "2020-01-10T07:42:14.967265Z"
    }
   },
   "outputs": [],
   "source": [
    "#on enregistre le dataset contenant les url\n",
    "export_df= df.to_csv(\"data/dataset_url_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T20:42:43.423571Z",
     "start_time": "2020-01-07T20:42:43.410607Z"
    }
   },
   "source": [
    "## Transformation de nos URL en lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:36.486542Z",
     "start_time": "2020-01-10T12:47:36.445345Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/dataset_url_lyrics.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:47:49.048274Z",
     "start_time": "2020-01-11T15:47:49.031318Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretransformation_lyric(url_son):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        #on se connecte et on récupère la partie de la page avec les lyrics\n",
    "        page= requests.get(url_son)\n",
    "        soup= BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        lyric= str(page.content)\n",
    "        pattern= re.compile('(?:Sorry about that. -->)([\\s\\S]*)(?:-- MxM)')\n",
    "        res= pattern.search(lyric).group(1)\n",
    "\n",
    "        #On va transformer notre texte de telle sorte qu'il soit exploitable par nos Lemmnizer / Stemmenizer\n",
    "        #J'encode en #!utf8 pour accepter les \\n etc\n",
    "        res= res.encode('utf8').decode(\"unicode_escape\")\n",
    "        #J'y ai mis une balise que j'ai du mal à supprimer\n",
    "        banword= ['br', 'div', 'brbr']\n",
    "        #On tokenize notre text\n",
    "        tokenizer= TreebankWordTokenizer()\n",
    "        #Je sépare les élément tokenizer\n",
    "        tokenz= [','.join(tokenizer.tokenize(mot)) for mot in res.split()]\n",
    "\n",
    "        tokenz= [mot.replace(\",\", \"\").replace(\"<br>\", \"\") for mot in tokenz]\n",
    "        #J'enlève ce qui n'est pas du texte ou en espace\n",
    "        tokenz= [re.sub('[^\\w\\s]', ' ', mot) for mot in tokenz]\n",
    "        #Je supprime les espaces inutiles\n",
    "        tokenz= [mot.replace(' ','') for mot in tokenz]\n",
    "        #Et enfin j'applique ma liste de banword\n",
    "    #     text_clean= [mot for mot in tokenz if mot not in banword]\n",
    "        text_clean= ''\n",
    "        for mot in tokenz:\n",
    "            if mot not in banword:\n",
    "                text_clean += mot + ' '\n",
    "\n",
    "        #on fait un sleep entre 5 et 30 sec\n",
    "        time.sleep(np.random.randint(5, 40, 1))\n",
    "\n",
    "        return text_clean #sortie de cette facon pour utiliser lemma de spacy\n",
    "        \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T17:11:22.876079Z",
     "start_time": "2020-01-08T17:11:22.871611Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# liste_lyrics= [pretransformation_lyric(url) \\\n",
    "#               for url in df[\"Url\"].values]\n",
    "# liste_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:44.813785Z",
     "start_time": "2020-01-10T12:47:44.793411Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "export_df= df.to_csv(\"random_dataset_url_lyrics.csv\", index= False, header= True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:48.115305Z",
     "start_time": "2020-01-10T12:47:48.106860Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Artiste\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:50.003396Z",
     "start_time": "2020-01-10T12:47:49.986802Z"
    }
   },
   "outputs": [],
   "source": [
    "#on va scinder notre dataset pour faire plusieurs set de web scraping\n",
    "df_1= df.iloc[:200,:]\n",
    "df_2= df.iloc[200:400,:]\n",
    "df_3= df.iloc[400:600,:]\n",
    "df_4= df.iloc[600:800,:]\n",
    "df_5= df.iloc[800:1000,:]\n",
    "df_6= df.iloc[1000:df.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T13:56:18.212269Z",
     "start_time": "2020-01-10T12:47:52.971368Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1[\"test\"]= df_1.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T13:56:18.266259Z",
     "start_time": "2020-01-10T13:56:18.238131Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_1.shape)\n",
    "verification(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T13:56:59.562299Z",
     "start_time": "2020-01-10T13:56:59.525274Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_1.to_csv(\"data/01_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:15:15.684207Z",
     "start_time": "2020-01-10T14:01:15.829062Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2[\"test\"]= df_2.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_2.shape)\n",
    "verification(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:22:18.209102Z",
     "start_time": "2020-01-10T15:22:18.175723Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_2.to_csv(\"data/02_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T16:51:23.760207Z",
     "start_time": "2020-01-10T15:39:26.779501Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3[\"test\"]= df_3.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_3.shape)\n",
    "verification(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T16:51:32.377222Z",
     "start_time": "2020-01-10T16:51:32.350342Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_3.to_csv(\"data/03_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:48:08.102270Z",
     "start_time": "2020-01-11T15:48:08.052405Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data/random_dataset_url_lyrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:48:08.504500Z",
     "start_time": "2020-01-11T15:48:08.496521Z"
    }
   },
   "outputs": [],
   "source": [
    "df_4= df.iloc[600:800,:]\n",
    "df_5= df.iloc[800:1000,:]\n",
    "df_6= df.iloc[1000:df.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:06:39.989656Z",
     "start_time": "2020-01-11T15:48:09.990091Z"
    }
   },
   "outputs": [],
   "source": [
    "df_4[\"test\"]= df_4.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_4.shape)\n",
    "verification(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:10:28.722261Z",
     "start_time": "2020-01-11T17:10:28.682330Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_4.to_csv(\"data/04_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:30:37.372359Z",
     "start_time": "2020-01-11T17:12:49.276356Z"
    }
   },
   "outputs": [],
   "source": [
    "df_5[\"test\"]= df_5.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_5.shape)\n",
    "verification(df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:33:57.107894Z",
     "start_time": "2020-01-11T18:33:57.072308Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_5.to_csv(\"data/05_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:43:42.222817Z",
     "start_time": "2020-01-11T18:34:13.904829Z"
    }
   },
   "outputs": [],
   "source": [
    "df_6[\"test\"]= df_6.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_6.shape)\n",
    "verification(df_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T19:21:00.365729Z",
     "start_time": "2020-01-11T19:21:00.350843Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_6.to_csv(\"data/06_dataset_lyrics.csv\", index= False, header= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
