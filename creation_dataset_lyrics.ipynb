{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche de lyrics par artiste\n",
    "\n",
    "Notre but est de récupérer par artiste, l'ensemble des paroles de ses sons.\n",
    "\n",
    "\n",
    "__ATTENTION__ ici je ne me concentre que sur les albums des artistes, c'est à dire que tous les song qui ne sont pas sorties dans un album ne sont pas comtabilisé. En effet, sans cette condition difficile de mettre une année sur la diffusion du son.\n",
    "\n",
    "\n",
    "Nous allons utiliser ce site [https://search.azlyrics.com](https://search.azlyrics.com) pour la recherche des paroles.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Voici donc les étapes de la constitution de cette base:\n",
    "\n",
    "-  Dans un premier temps, la fonction <a href='#artist_to_url'>artist_to_url</a> nous permet de générer à l'aide d'un nom d'artiste un url vers l'ensemble de sa discographie.\n",
    "-  Notre deuxième fonction <a href='#album_url_year'>album_url_year</a> va nous permettre de ressortir JE NE SAIS PAS ENCORE SOUS QUEL FORMAT le nom de l'artiste, le nom de l'album, l'année de difusion et enfin les liens vers les musiques contenus dans les albums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:39:16.035343Z",
     "start_time": "2020-01-24T08:39:13.948717Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "from lxml import html\n",
    "import sys\n",
    "\n",
    "# from nltk.stem.snowball import EnglishStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:39:18.197584Z",
     "start_time": "2020-01-24T08:39:16.040539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wenceslas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:39:18.218228Z",
     "start_time": "2020-01-24T08:39:18.202000Z"
    }
   },
   "outputs": [],
   "source": [
    "#permet de générer l'url pour accéder à l'ensemble des titres de l'artiste\n",
    "#certain artiste comme Renaud ne sont pas référencé\n",
    "def artist_to_url(artist):\n",
    "    \"\"\" A partir d'un nom d'artiste, on produit un URL pour la connexion vers le site \n",
    "        `https://www.azlyrics.com`. \n",
    "        \n",
    "    On a remarqué qu'il existait un pattern pour\n",
    "    générer un URL à partir d'un artiste.\n",
    "    En effet, si le premier caractère du nom est une lettre, alors l'URL se construit\n",
    "    de la façon suivante:\n",
    "        https://www.azlyrics.com/{première lettre du nom de l'artiste}/{le nom de l'artiste}\n",
    "    Sinon:\n",
    "        https://www.azlyrics.com/19/{le nom de l'artiste}\n",
    "        \n",
    "    Args:\n",
    "        -artist: désigne le nom de l'artiste\n",
    "    \n",
    "    Résultat:\n",
    "        >>> artist_to_url(\"Schoolboy Q\")\n",
    "        >>> https://www.azlyrics.com/s/schoolboyq.html'\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    url_base= \"\"\"https://www.azlyrics.com\"\"\"\n",
    "    name= re.sub(' ','',artist)\n",
    "    # Enlever les accents comme é ou â\n",
    "    name= unidecode.unidecode(name.lower())\n",
    "    first_l= list(name)[0]\n",
    "    # Certains artistes ont un nom qui commence par un chiffre\n",
    "    # dans ce cas, la procédure de création de l'url est un peu différent\n",
    "    if first_l.isalpha():\n",
    "        url= \"\"\"{}/{}/{}.html\"\"\".format(url_base, first_l, name)\n",
    "    else:\n",
    "        url= \"\"\"{}/19/{}.html\"\"\".format(url_base, name)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:39:18.243061Z",
     "start_time": "2020-01-24T08:39:18.223295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.azlyrics.com/s/schoolboyq.html'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "artist_to_url(\"Schoolboy Q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:39:18.259420Z",
     "start_time": "2020-01-24T08:39:18.249780Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_proxy_http(proxy):\n",
    "    \"\"\" A partir d'un proxy (avec son port), on cherche à générer une adresse de connexion http\n",
    "        \n",
    "    Args:\n",
    "        -proxy\n",
    "        \n",
    "    Résultat:\n",
    "        >>> transform_proxy_http(\"91.228.8.162:8080\")\n",
    "        >>> \"http://91.228.8.162:8080\"\n",
    "        \n",
    "    Raises:\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    return \"http://\"+proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-19T12:11:27.155Z"
    }
   },
   "outputs": [],
   "source": [
    "# On récupère une liste de proxy (mise à jour quotidienne)\n",
    "urll= \"https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt\"\n",
    "urllib.request.urlretrieve(urll, 'data/proxy_list.txt')\n",
    "\n",
    "listed_proxy= []\n",
    "f= open(\"data/proxy_list.txt\", \"r\")\n",
    "listed_proxy= f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-19T12:11:28.169Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On cherche à faire une pré-sélection des proxy \n",
    "# On teste donc leur connexion sur le site qu'on cherche à scaper\n",
    "\n",
    "good_prox= []\n",
    "# Pour raison de simpliciter, on cherche que les proxy qui ont un port 8080\n",
    "pat= re.compile('.:8080$')\n",
    "\n",
    "proxies_list= [l for l in listed_proxy \\\n",
    "               if l in list(filter(pat.findall, listed_proxy))]\n",
    "\n",
    "for prox in proxies_list:\n",
    "    # On cherche 20 proxy de bonnes qualités\n",
    "    if len(good_prox) <= 20:\n",
    "        try:\n",
    "            print(prox)\n",
    "            \n",
    "            #On génère un User Agent aléatoire pour chaque proxy\n",
    "            software_names = [SoftwareName.CHROME.value]\n",
    "            operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]\n",
    "            user_agent_rotator = UserAgent(software_names=software_names,\n",
    "                           operating_systems=operating_systems, limit=100)\n",
    "            \n",
    "            proxies= {\"http\":transform_proxy_http(prox),\n",
    "                    \"https\":transform_proxy_http(prox)}\n",
    "\n",
    "            user_agent = user_agent_rotator.get_random_user_agent()\n",
    "            headers= {\"User-Agent\":user_agent}\n",
    "\n",
    "            r = requests.Session()\n",
    "            r.headers.update(headers)\n",
    "            r.proxies.update(proxies)\n",
    "\n",
    "            # Connexion à la page\n",
    "            page= r.get(\"https://www.azlyrics.com/\", proxies= proxies, headers= headers)\n",
    "\n",
    "            # Si la connexion est fructueuse, alors le proxy est stocké\n",
    "            good_prox.append(prox)\n",
    "        except:\n",
    "            # Si je ne peux pas me connecter avec ce proxy, alors je teste le suivant\n",
    "            print(\"Not Good\")\n",
    "            continue\n",
    "    else:\n",
    "        # Si j'ai 20 bon proxy, j'arrète la sélection\n",
    "        break\n",
    "print(\"Fin\")\n",
    "\n",
    "good_prox[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:47:15.257775Z",
     "start_time": "2020-01-24T08:47:15.214921Z"
    }
   },
   "outputs": [],
   "source": [
    "def album_url_year(artist, proxies_list= None, user_agent= True):\n",
    "    \"\"\"Permet de générer pour un artiste un dataframe contant les informations sur ses\n",
    "    productions.\n",
    "    \n",
    "    A partir du nom de l'artiste, cette fonction récupère tous les albums produit par l'artiste\n",
    "    (qui sont disponibles sur le site https://www.azlyrics.com) auquel est associé \n",
    "    une année de publication et une liste d'URL menant vers les paroles des musiques de cet album.\n",
    "    On laisse le choix à l'utilisateur d'utiliser une liste de proxy pour requêter.\n",
    "        \n",
    "    Args:\n",
    "        -artist: le nom de l'artiste\n",
    "        -proxies_list: une liste de proxy, utilise si on veut scraper une groose\n",
    "            quantité d'artiste. Par défaut, cet argument prend la valeur `None` car le fait d'utiliser\n",
    "            les proxy ralentit énormément le temps de requétage.\n",
    "        -user_agent: si l'utilisateur souhaite, génére aléatoirement un UserAgent (utile pour scraper\n",
    "            de grosse quantité). Par défaut, on génère un UserAgent.\n",
    "        \n",
    "    Résultat:\n",
    "        >>> album_url_year(\"jinjer\")\n",
    "        >>> \tArtiste\tAnnee\tAlbum\tUrl\n",
    "            0\tjinjer\t2012\tInhale. Do Not Breathe\thttps://www.azlyrics.com/lyrics/jinjer/untilth...\n",
    "            1\tjinjer\t2012\tInhale. Do Not Breathe\thttps://www.azlyrics.com/lyrics/jinjer/waltz.html\n",
    "            2\tjinjer\t2012\tInhale. Do Not Breathe\thttps://www.azlyrics.com/lyrics/jinjer/scissor...\n",
    "            3\tjinjer\t2012\tInhale. Do Not Breathe\thttps://www.azlyrics.com/lyrics/jinjer/exposed...\n",
    "            4\tjinjer\t2012\tInhale. Do Not Breathe\thttps://www.azlyrics.com/lyrics/jinjer/mylostc...\n",
    "            5\tjinjer\t2014\tCloud Factory     \t    https://www.azlyrics.com/lyrics/jinjer/outland...\n",
    "            6\tjinjer\t2014\tCloud Factory    \t    https://www.azlyrics.com/lyrics/jinjer/aplusor...\n",
    "    \n",
    "    Raises:\n",
    "        -warning si on ne peut pas construire le DataFrame (vérifier la longueur des listes qu'on\n",
    "            utilise pour construire notre DataFrame)\n",
    "        -warning si on n'arrive pas à trouver l'artiste dans le site (l'artiste n'existe pas,\n",
    "            le pattern de l'artiste n'a pas bien été géré, trop d'erreurs liées aux proxies etc.)\n",
    "        -warning si le proxy ne nous permet pas de nous connecter à la page\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    year, album_name, url_song= [], [], []\n",
    "    \n",
    "    if user_agent:\n",
    "        software_names = [SoftwareName.CHROME.value]\n",
    "        operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]\n",
    "        user_agent_rotator = UserAgent(software_names= software_names,\n",
    "                                   operating_systems= operating_systems, limit= 100)\n",
    "        \n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "        headers= {\"User-Agent\":user_agent}\n",
    "        \n",
    "        r = requests.Session()\n",
    "        r.headers.update(headers)\n",
    "    \n",
    "    else:\n",
    "        r = requests.Session()\n",
    "        headers= None\n",
    "        \n",
    "    if proxies_list:\n",
    "        \n",
    "        random_proxy= sorted(proxies_list, key=lambda x: random())\n",
    "        i = 0\n",
    "        for prox in random_proxy:\n",
    "            if i < 5: #si trop de fail\n",
    "                i += 1\n",
    "                try:\n",
    "\n",
    "                    proxies= {\"http\":transform_proxy_http(prox),\n",
    "                            \"https\":transform_proxy_http(prox)}\n",
    "\n",
    "                    r.proxies.update(proxies)\n",
    "\n",
    "                    page= r.get(artist_to_url(artist), proxies= proxies, headers= headers)\n",
    "\n",
    "                    soup= BeautifulSoup(page.text, 'html.parser')\n",
    "                    html_page= soup.find('div', id= 'listAlbum')\n",
    "\n",
    "                    html_data= str(html_page).split('<div class=\"album\"')[1:][:-1]\n",
    "                    for ht in html_data:\n",
    "                    #je défini mes patterns REGEX pour récupérer de la page html le nom des albums, avec son année et les url des lyrics menant aux paroles de chacunes des musiques de l'album     \n",
    "                        pattern_yr= re.compile('</b> \\((\\d{4})\\)</div>|</b> \\((\\d{4})\\)<br/>')\n",
    "                        pattern_alb= re.compile('<b>\"([\\s\\S]*)\"</b>')\n",
    "                        pattern_url= re.compile('href=\"..([\\s\\S]*).html\" target=')\n",
    "\n",
    "                        year.append(pattern_yr.search(ht).group(1))\n",
    "                        album_name.append(pattern_alb.search(ht).group(1))\n",
    "                        #le traitement pour les url est un peu différent\n",
    "                        url_list= ht.split('\\n')\n",
    "                        url_base= \"https://www.azlyrics.com\"\n",
    "                        url_song.append([url_base + pattern_url.search(href).group(1) + \".html\" \\\n",
    "                                for href in url_list \\\n",
    "                                if pattern_url.search(href) != None])\n",
    "                    try:\n",
    "                        df= pd.DataFrame(\n",
    "                        {\n",
    "                            \"Artiste\": [artist]*len(year),\n",
    "                            \"Annee\": year,\n",
    "                            \"Album\": album_name,\n",
    "                            \"Url\": url_song\n",
    "                        })\n",
    "                    except:\n",
    "                        warnings.warn(\"Attention, il y a un problème dans la construction du DataFrame\")\n",
    "                        return None\n",
    "\n",
    "                    #le df que je creer contient en URF une liste des url, je veux donc avoir une ligne par URL et donc\n",
    "                    #les valeurs annee album, et artiste qui correspondent.\n",
    "                    df_not_listed= pd.DataFrame({\n",
    "                      col:np.repeat(df[col].values, df[\"Url\"].str.len())\n",
    "                      for col in df.columns.drop(\"Url\")\n",
    "                    }\n",
    "                    ).assign(**{\n",
    "                        \"Url\":np.concatenate(df[\"Url\"].values)\n",
    "                    })[df.columns]\n",
    "\n",
    "                    return df_not_listed\n",
    "                except:\n",
    "                    warnings.warn(\"Attention, le proxy {} n'a pas permis de vous connecter à \\\n",
    "                                  la page souhaitée\".format(prox))\n",
    "                    continue\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        warnings.warn(\"Attention, l'artiste {} n'a pas été trouvé\".format(artist))\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        page= r.get(artist_to_url(artist))\n",
    "#         page= requests.get(artist_to_url(artist)) #l'appelle la fonction précédente\n",
    "        soup= BeautifulSoup(page.text, 'html.parser')\n",
    "        html_page= soup.find('div', id= 'listAlbum')\n",
    "        try:\n",
    "            html_data= str(html_page).split('<div class=\"album\"')[1:][:-1]\n",
    "            for ht in html_data:\n",
    "            #je défini mes patterns REGEX pour récupérer de la page html le nom des albums, avec son année et les url des lyrics menant aux paroles de chacunes des musiques de l'album     \n",
    "                pattern_yr= re.compile('</b> \\((\\d{4})\\)</div>|</b> \\((\\d{4})\\)<br/>')\n",
    "                pattern_alb= re.compile('<b>\"([\\s\\S]*)\"</b>')\n",
    "                pattern_url= re.compile('href=\"..([\\s\\S]*).html\" target=')\n",
    "\n",
    "                year.append(pattern_yr.search(ht).group(1))\n",
    "                album_name.append(pattern_alb.search(ht).group(1))\n",
    "                #le traitement pour les url est un peu différent\n",
    "                url_list= ht.split('\\n')\n",
    "                url_base= \"https://www.azlyrics.com\"\n",
    "                url_song.append([url_base + pattern_url.search(href).group(1) + \".html\" \\\n",
    "                        for href in url_list \\\n",
    "                        if pattern_url.search(href) != None])\n",
    "            try:\n",
    "                df= pd.DataFrame(\n",
    "                {\n",
    "                    \"Artiste\": [artist]*len(year),\n",
    "                    \"Annee\": year,\n",
    "                    \"Album\": album_name,\n",
    "                    \"Url\": url_song\n",
    "                })\n",
    "                \n",
    "            except:\n",
    "                warnings.warn(\"Attention, il y a un problème dans la construction du DataFrame\")\n",
    "                return None\n",
    "\n",
    "            #le df que je creer contient en URF une liste des url, je veux donc avoir une ligne par URL et donc\n",
    "            #les valeurs annee album, et artiste qui correspondent.\n",
    "            df_not_listed= pd.DataFrame({\n",
    "              col:np.repeat(df[col].values, df[\"Url\"].str.len())\n",
    "              for col in df.columns.drop(\"Url\")\n",
    "            }\n",
    "            ).assign(**{\n",
    "                \"Url\":np.concatenate(df[\"Url\"].values)\n",
    "            })[df.columns]\n",
    "\n",
    "            return df_not_listed\n",
    "        \n",
    "        except:\n",
    "            warnings.warn(\"Attention, l'artiste {} n'a pas été trouvé\".format(artist))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:08:34.458626Z",
     "start_time": "2020-01-24T08:08:34.450852Z"
    }
   },
   "outputs": [],
   "source": [
    "album_url_year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T08:47:17.367085Z",
     "start_time": "2020-01-24T08:47:17.193200Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenceslas\\Anaconda3\\envs\\cours\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Attention, l'artiste janjer n'a pas été trouvé\n"
     ]
    }
   ],
   "source": [
    "album_url_year(\"janjer\", user_agent= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-01-19T10:11:58.609Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "artiste_name_style = {\n",
    "    \"Slipknot\":\"rock\",\n",
    "    \"DREAMERS\":\"rock\",\n",
    "    \"Bring Me The Horizon\":\"rock\",\n",
    "    \"Motionless in White\":\"rock\",\n",
    "    \"Falling In Reverse\":\"rock\",\n",
    "    \"Eminem\":\"rap\",\n",
    "    \"Justin Bieber\":\"pop\",\n",
    "    \"Post Malone\":\"rap\",\n",
    "    \"Billie Eilish\":\"pop\",\n",
    "    \"Mac Miller\":\"rap\",\n",
    "}\n",
    "artiste_name= [name for name in artiste_name_style.keys()]\n",
    "\n",
    "artist_random= sorted(artist_name, key=lambda x: random())\n",
    "df= pd.DataFrame()\n",
    "for art in artist_random:\n",
    "    print(\"\\n\"+art)\n",
    "    df= pd.concat([album_url_year(art), df], ignore_index= True)\n",
    "\n",
    "print(\"\\nFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:39:09.354704Z",
     "start_time": "2020-01-17T09:39:09.336487Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ajout colonne Style à partir du dictionnaire\n",
    "df[\"Style\"]= df[\"Artiste\"].map(artiste_name_style)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:39:08.150396Z",
     "start_time": "2020-01-17T09:39:08.137326Z"
    }
   },
   "outputs": [],
   "source": [
    "def verification(df):\n",
    "    print(\"Vérifier si des valeurs sont None\")\n",
    "    for col in df.columns:\n",
    "        nul= (df[col].isnull()).sum()\n",
    "        sh= df.shape[0]\n",
    "        print(\"\\n\")\n",
    "        print(col)\n",
    "        print(nul)\n",
    "        print(\"Soit: {}% de valeurs None\".format((nul/sh)*100))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:39:46.180697Z",
     "start_time": "2020-01-17T09:39:46.169729Z"
    }
   },
   "outputs": [],
   "source": [
    "verification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T09:39:57.155880Z",
     "start_time": "2020-01-17T09:39:57.148023Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T07:42:14.992242Z",
     "start_time": "2020-01-10T07:42:14.967265Z"
    }
   },
   "outputs": [],
   "source": [
    "#on enregistre le dataset contenant les url\n",
    "export_df= df.to_csv(\"data/dataset_url_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T20:42:43.423571Z",
     "start_time": "2020-01-07T20:42:43.410607Z"
    }
   },
   "source": [
    "## Transformation de nos URL en lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:36.486542Z",
     "start_time": "2020-01-10T12:47:36.445345Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/dataset_url_lyrics.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:47:49.048274Z",
     "start_time": "2020-01-11T15:47:49.031318Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretransformation_lyric(url_son, proxies_list= None, user_agent= True):\n",
    "    \"\"\"Récupère les contenus d'une page web à partir d'un URL\n",
    "    \n",
    "    BB\n",
    "    \n",
    "    Args:\n",
    "        -url_son: url menant vers la page web d'une musique, contenant ainsi les paroles\n",
    "            de la musique en question\n",
    "        -proxies_list: une liste de proxy, utilise si on veut scraper une groose\n",
    "            quantité d'artiste. Par défaut, cet argument prend la valeur `None` car le fait d'utiliser\n",
    "            les proxy ralentit énormément le temps de requétage.\n",
    "        -user_agent: si l'utilisateur souhaite, génére aléatoirement un UserAgent (utile pour scraper\n",
    "            de grosse quantité). Par défaut, on génère un UserAgent.\n",
    "    \n",
    "    Raises:\n",
    "    \n",
    "    \"\"\"\n",
    "    if user_agent:\n",
    "        software_names = [SoftwareName.CHROME.value]\n",
    "        operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]\n",
    "        user_agent_rotator = UserAgent(software_names= software_names,\n",
    "                                   operating_systems= operating_systems, limit= 100)\n",
    "        \n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "        headers= {\"User-Agent\":user_agent}\n",
    "        \n",
    "        r = requests.Session()\n",
    "        r.headers.update(headers)\n",
    "    \n",
    "    else:\n",
    "        r = requests.Session()\n",
    "        headers= None\n",
    "        \n",
    "    if proxies_list:\n",
    "        random_proxy= sorted(proxies_list, key=lambda x: random())\n",
    "        for prox in random_proxy:\n",
    "            try:\n",
    "                proxies= {\"http\":transform_proxy_http(prox),\n",
    "                        \"https\":transform_proxy_http(prox)}\n",
    "                r.proxies.update(proxies)\n",
    "\n",
    "                page= r.get(url_son, proxies= proxies, headers= headers)\n",
    "\n",
    "    #             page= requests.get(url_son)\n",
    "                soup= BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "                lyric= str(page.content)\n",
    "                pattern= re.compile('(?:Sorry about that. -->)([\\s\\S]*)(?:-- MxM)')\n",
    "                res= pattern.search(lyric).group(1)\n",
    "\n",
    "                #On va transformer notre texte de telle sorte qu'il soit exploitable par nos Lemmnizer / Stemmenizer\n",
    "                #J'encode en #!utf8 pour accepter les \\n etc\n",
    "                res= res.encode('utf8').decode(\"unicode_escape\")\n",
    "                #J'y ai mis une balise que j'ai du mal à supprimer\n",
    "                banword= ['br', 'div', 'brbr']\n",
    "                #On tokenize notre text\n",
    "                tokenizer= TreebankWordTokenizer()\n",
    "                #Je sépare les élément tokenizer\n",
    "                tokenz= [','.join(tokenizer.tokenize(mot)) for mot in res.split()]\n",
    "\n",
    "                tokenz= [mot.replace(\",\", \"\").replace(\"<br>\", \"\") for mot in tokenz]\n",
    "                #J'enlève ce qui n'est pas du texte ou en espace\n",
    "                tokenz= [re.sub('[^\\w\\s]', ' ', mot) for mot in tokenz]\n",
    "                #Je supprime les espaces inutiles\n",
    "                tokenz= [mot.replace(' ','') for mot in tokenz]\n",
    "                #Et enfin j'applique ma liste de banword\n",
    "            #     text_clean= [mot for mot in tokenz if mot not in banword]\n",
    "                text_clean= ''\n",
    "                for mot in tokenz:\n",
    "                    if mot not in banword:\n",
    "                        text_clean += mot + ' '\n",
    "\n",
    "\n",
    "                return text_clean #sortie de cette facon pour utiliser lemma de spacy\n",
    "\n",
    "            except:\n",
    "                warnings.warn(\"Attention, le proxy {} n'a pas permis de vous connecter à \\\n",
    "                                  la page souhaitée\".format(prox))\n",
    "                continue\n",
    "    else:\n",
    "        page= r.get(url_son, proxies= proxies, headers= headers)\n",
    "\n",
    "        soup= BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        lyric= str(page.content)\n",
    "        pattern= re.compile('(?:Sorry about that. -->)([\\s\\S]*)(?:-- MxM)')\n",
    "        res= pattern.search(lyric).group(1)\n",
    "\n",
    "        #On va transformer notre texte de telle sorte qu'il soit exploitable par nos Lemmnizer / Stemmenizer\n",
    "        #J'encode en #!utf8 pour accepter les \\n etc\n",
    "        res= res.encode('utf8').decode(\"unicode_escape\")\n",
    "        #J'y ai mis une balise que j'ai du mal à supprimer\n",
    "        banword= ['br', 'div', 'brbr']\n",
    "        #On tokenize notre text\n",
    "        tokenizer= TreebankWordTokenizer()\n",
    "        #Je sépare les élément tokenizer\n",
    "        tokenz= [','.join(tokenizer.tokenize(mot)) for mot in res.split()]\n",
    "\n",
    "        tokenz= [mot.replace(\",\", \"\").replace(\"<br>\", \"\") for mot in tokenz]\n",
    "        #J'enlève ce qui n'est pas du texte ou en espace\n",
    "        tokenz= [re.sub('[^\\w\\s]', ' ', mot) for mot in tokenz]\n",
    "        #Je supprime les espaces inutiles\n",
    "        tokenz= [mot.replace(' ','') for mot in tokenz]\n",
    "        #Et enfin j'applique ma liste de banword\n",
    "    #     text_clean= [mot for mot in tokenz if mot not in banword]\n",
    "        text_clean= ''\n",
    "        for mot in tokenz:\n",
    "            if mot not in banword:\n",
    "                text_clean += mot + ' '\n",
    "\n",
    "\n",
    "        return text_clean #sortie de cette facon pour utiliser lemma de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T17:11:22.876079Z",
     "start_time": "2020-01-08T17:11:22.871611Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# liste_lyrics= [pretransformation_lyric(url) \\\n",
    "#               for url in df[\"Url\"].values]\n",
    "# liste_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:44.813785Z",
     "start_time": "2020-01-10T12:47:44.793411Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "export_df= df.to_csv(\"random_dataset_url_lyrics.csv\", index= False, header= True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:48.115305Z",
     "start_time": "2020-01-10T12:47:48.106860Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Artiste\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T12:47:50.003396Z",
     "start_time": "2020-01-10T12:47:49.986802Z"
    }
   },
   "outputs": [],
   "source": [
    "#on va scinder notre dataset pour faire plusieurs set de web scraping\n",
    "df_1= df.iloc[:200,:]\n",
    "df_2= df.iloc[200:400,:]\n",
    "df_3= df.iloc[400:600,:]\n",
    "df_4= df.iloc[600:800,:]\n",
    "df_5= df.iloc[800:1000,:]\n",
    "df_6= df.iloc[1000:df.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T13:56:18.212269Z",
     "start_time": "2020-01-10T12:47:52.971368Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1[\"test\"]= df_1.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T13:56:18.266259Z",
     "start_time": "2020-01-10T13:56:18.238131Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_1.shape)\n",
    "verification(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T13:56:59.562299Z",
     "start_time": "2020-01-10T13:56:59.525274Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_1.to_csv(\"data/01_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:15:15.684207Z",
     "start_time": "2020-01-10T14:01:15.829062Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2[\"test\"]= df_2.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_2.shape)\n",
    "verification(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T15:22:18.209102Z",
     "start_time": "2020-01-10T15:22:18.175723Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_2.to_csv(\"data/02_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T16:51:23.760207Z",
     "start_time": "2020-01-10T15:39:26.779501Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3[\"test\"]= df_3.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_3.shape)\n",
    "verification(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T16:51:32.377222Z",
     "start_time": "2020-01-10T16:51:32.350342Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_3.to_csv(\"data/03_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:48:08.102270Z",
     "start_time": "2020-01-11T15:48:08.052405Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data/random_dataset_url_lyrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:48:08.504500Z",
     "start_time": "2020-01-11T15:48:08.496521Z"
    }
   },
   "outputs": [],
   "source": [
    "df_4= df.iloc[600:800,:]\n",
    "df_5= df.iloc[800:1000,:]\n",
    "df_6= df.iloc[1000:df.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:06:39.989656Z",
     "start_time": "2020-01-11T15:48:09.990091Z"
    }
   },
   "outputs": [],
   "source": [
    "df_4[\"test\"]= df_4.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_4.shape)\n",
    "verification(df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:10:28.722261Z",
     "start_time": "2020-01-11T17:10:28.682330Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_4.to_csv(\"data/04_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:30:37.372359Z",
     "start_time": "2020-01-11T17:12:49.276356Z"
    }
   },
   "outputs": [],
   "source": [
    "df_5[\"test\"]= df_5.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_5.shape)\n",
    "verification(df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:33:57.107894Z",
     "start_time": "2020-01-11T18:33:57.072308Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_5.to_csv(\"data/05_dataset_lyrics.csv\", index= False, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T18:43:42.222817Z",
     "start_time": "2020-01-11T18:34:13.904829Z"
    }
   },
   "outputs": [],
   "source": [
    "df_6[\"test\"]= df_6.apply(lambda row: pretransformation_lyric(row[3])\n",
    "                     ,axis= 1)\n",
    "#pour gérer les erreurs, tester si la page nous a repéré, si oui ==> capcha ou acces denied ?\n",
    "print(df_6.shape)\n",
    "verification(df_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T19:21:00.365729Z",
     "start_time": "2020-01-11T19:21:00.350843Z"
    }
   },
   "outputs": [],
   "source": [
    "export_df= df_6.to_csv(\"data/06_dataset_lyrics.csv\", index= False, header= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
