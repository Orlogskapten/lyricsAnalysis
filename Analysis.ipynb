{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Analysis\n",
    "We scrapped the AZLyrics website in the previous Jupyter Notebook, and are now going to use the collected data for Lyrics analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:46.537747Z",
     "start_time": "2020-02-04T08:47:44.288507Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:46.657691Z",
     "start_time": "2020-02-04T08:47:46.541172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import and merge the csv files generated during the data collection\n",
    "df= pd.DataFrame()\n",
    "for i in range(6):\n",
    "    try: # if file exists\n",
    "        df= pd.concat([df, pd.read_csv(\"data/0{}_dataset_lyrics.csv\".format(i+1))])\n",
    "    except:\n",
    "        #au cas ou , si mon fichier n'existe pas alors j'ajoute un df vide\n",
    "        df= pd.concat([df, pd.DataFrame()])\n",
    "        \n",
    "df= df.rename({\"test\":\"Lyrics\"}, axis=1)\n",
    "df.drop(columns='Url', inplace=True)\n",
    "df.loc[df['Style'] == 'Rock\\\\Alternatif', 'Style'] = 'Rock/Alternatif' # Fixing a small issue with the dataset\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:46.673426Z",
     "start_time": "2020-02-04T08:47:46.660811Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:46.698748Z",
     "start_time": "2020-02-04T08:47:46.676826Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have successfully scrapped the lyrics of over 1 thousand songs. We also extracted interesting meta data which are going to enhance our analysis:\n",
    "- Artist\n",
    "- Year\n",
    "- Album name\n",
    "- Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:47.163955Z",
     "start_time": "2020-02-04T08:47:46.702645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the lyrics into arrays of words\n",
    "df['words'] = df.apply(\n",
    "    lambda row: np.array(re.findall(r'\\w+', row['Lyrics'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:47.347664Z",
     "start_time": "2020-02-04T08:47:47.167734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Erase the stop words\n",
    "stop_words = np.array(set(stopwords.words('english')))\n",
    "df['words'] = df['words'].apply(\n",
    "    lambda word_array: word_array[np.logical_not(np.isin(word_array, stop_words))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:47.370035Z",
     "start_time": "2020-02-04T08:47:47.350662Z"
    }
   },
   "outputs": [],
   "source": [
    "df['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Lyrics length\n",
    "The first interesting insight that we can get from our dataset is the number of words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:47.386933Z",
     "start_time": "2020-02-04T08:47:47.374159Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_words'] = df['words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:47.947480Z",
     "start_time": "2020-02-04T08:47:47.389913Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    words_per_year = df[df['Style'] == style].groupby('Annee')['n_words'].mean()\n",
    "    sns.lineplot(words_per_year.index, words_per_year.values, label=style)\n",
    "plt.title('Various styles mean number of words')\n",
    "plt.ylabel('Mean number of words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:49.626848Z",
     "start_time": "2020-02-04T08:47:47.950777Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(16, 12))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    for artist in df.loc[df['Style'] == style, 'Artiste'].unique():\n",
    "        words_per_year = df[df['Artiste'] == artist].groupby('Annee')['n_words'].mean()\n",
    "        sns.lineplot(words_per_year.index, words_per_year.values, label=artist, ax=ax[i])\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_xlabel('')\n",
    "plt.suptitle('Mean number of words per artist and per style')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of our music style have a homogeneous number of words around 300, except for the rap genre that is near double, around 600 words per music!\n",
    "\n",
    "Thus, **rap music generally includes more words per song than other music styles**.\n",
    "\n",
    "Nevertheless, **there is more heterogeneity in the rap style than in other styles**. For example, Eminem produces music with way more lyrics than Post Malone, even though they are both considered rappers.\n",
    "\n",
    "We should now have a look at the diversity of these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Unique words\n",
    "Now that we know the sizes of lyrics, let's have a look at their diversity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:49.873574Z",
     "start_time": "2020-02-04T08:47:49.629545Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_unique_words'] = df['words'].apply(\n",
    "    lambda word_list: len(set(word_list))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:50.408142Z",
     "start_time": "2020-02-04T08:47:49.875655Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    unique_words_per_year = df[df['Style'] == style].groupby('Annee')['n_unique_words'].mean()\n",
    "    sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=style)\n",
    "plt.title('Various styles mean number of unique words')\n",
    "plt.ylabel('Mean number of words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:51.993595Z",
     "start_time": "2020-02-04T08:47:50.409155Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(16, 12))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    for artist in df.loc[df['Style'] == style, 'Artiste'].unique():\n",
    "        unique_words_per_year = df[df['Artiste'] == artist].groupby('Annee')['n_unique_words'].mean()\n",
    "        sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=artist, ax=ax[i])\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_xlabel('')\n",
    "plt.suptitle('Mean number of unique words per artist and per style')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that, once again, the rap style uses more diverse words than the other music styles. This is no surprise, since it also uses more words (even non-unique) per songs, so we are going to look at the uniqueness ratio of the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:52.022188Z",
     "start_time": "2020-02-04T08:47:51.996952Z"
    }
   },
   "outputs": [],
   "source": [
    "df['unique_words_ratio'] = df['n_unique_words'] / df['n_words'] # Using highly optimized Numpy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:52.557606Z",
     "start_time": "2020-02-04T08:47:52.026590Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    unique_words_per_year = df[df['Style'] == style].groupby('Annee')['unique_words_ratio'].mean()\n",
    "    sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=style)\n",
    "plt.title('Various styles mean ratio of unique words')\n",
    "plt.ylabel('Mean number of words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is interesting! We can see two insights here:\n",
    "- Even though rap produces a higher quantity of lyrics, **the ratio of unique words is not largely higher than other genres**.\n",
    "- There are visible trends in the ratio of unique words:\n",
    " - **Metal diversity of vocabulary decline sinced 1995**.\n",
    " - **Pop diversity of vocabulary is rapidly increasing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:47:54.242161Z",
     "start_time": "2020-02-04T08:47:52.559600Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(16, 12))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    for artist in df.loc[df['Style'] == style, 'Artiste'].unique():\n",
    "        unique_words_per_year = df[df['Artiste'] == artist].groupby('Annee')['unique_words_ratio'].mean()\n",
    "        sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=artist, ax=ax[i])\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_xlabel('')\n",
    "plt.suptitle('Mean ratio of unique words per artist and per style')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the rapid increase of ratio of unique words comes from Billie Eilish's work. Is this a general trend in the pop genre, or is this a specificity of our dataset? We would need more data to confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Which words?\n",
    "Now that we have analyzed the quantities of words in the various lyrics of our dataset, we are going to have a qualitative approach, and understand which words are being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Most commonly used words\n",
    "First, we are going to have a look at the most commonly used words. Nevertheless, we want to consider the various declinations of a word into one single word, not many. We are thus going to stem our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:48:02.510769Z",
     "start_time": "2020-02-04T08:47:54.244892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stemming the words before counting\n",
    "stemmer = EnglishStemmer()\n",
    "df['stem_words'] = df['words'].apply(\n",
    "    lambda word_list: [stemmer.stem(word) for word in word_list] #TODO: optimize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:48:02.712249Z",
     "start_time": "2020-02-04T08:48:02.513516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Counting the used words\n",
    "all_words = pd.DataFrame({'word': [word for row in df['stem_words'] for word in row], 'count': 1})\n",
    "all_words = all_words.groupby('word').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T08:58:11.397440Z",
     "start_time": "2020-02-04T08:58:10.197682Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.stem(all_words.sort_values('count', ascending=False)[:50].index,\n",
    "        all_words.sort_values('count', ascending=False)[:50])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Most comonly used words')\n",
    "plt.ylabel('Number of use')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:07:26.770707Z",
     "start_time": "2020-02-04T09:07:21.575267Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(14,10))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    # Counting the used words\n",
    "    all_words = pd.DataFrame({'word': [word for row in df.loc[df['Style']==style, 'stem_words'] for word in row], 'count': 1})\n",
    "    all_words = all_words.groupby('word').sum()\n",
    "    ax[i].stem(\n",
    "        all_words.sort_values('count', ascending=False)[:50].index,\n",
    "        all_words.sort_values('count', ascending=False)[:50]\n",
    "    )\n",
    "    ax[i].xaxis.set_tick_params(rotation=90)\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_ylabel('Number of use')\n",
    "plt.suptitle('Most commonly used word per style')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Vulgarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to have a look at the vulgarity of the lyrics of our dataset.\n",
    "\n",
    "In order to do so, we are going to use a [ban word list](https://www.freewebheaders.com/bad-words-list-and-page-moderation-words-list-for-facebook/) which is used for facebook moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:14:01.629197Z",
     "start_time": "2020-02-04T09:14:01.613924Z"
    }
   },
   "outputs": [],
   "source": [
    "ban_words = pd.read_csv(\"data/ban_word.csv\", header= None, names= [\"nono\"])\n",
    "ban_words = ban_words.drop(index= 0).reset_index(drop= True)\n",
    "ban_words = np.array(ban_words.values)\n",
    "print(f'There are {len(ban_words)} banned words in our list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:17:06.169164Z",
     "start_time": "2020-02-04T09:16:52.591453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the banned_words\n",
    "df['banned_words'] = df['words'].apply(\n",
    "    lambda word_array: word_array[np.isin(word_array, ban_words)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:17:21.578769Z",
     "start_time": "2020-02-04T09:17:21.572359Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_banned_words'] = df['banned_words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:18:09.291383Z",
     "start_time": "2020-02-04T09:18:08.772331Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    words_per_year = df[df['Style'] == style].groupby('Annee')['n_banned_words'].mean()\n",
    "    sns.lineplot(words_per_year.index, words_per_year.values, label=style)\n",
    "plt.title('Mean number of banned words')\n",
    "plt.ylabel('Mean number of banned words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the rap music style is, once again, the highest one on the charts, but this is caused by the cheer number of word in their songs. Let's have a look at the ratio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:20:04.941336Z",
     "start_time": "2020-02-04T09:20:04.935321Z"
    }
   },
   "outputs": [],
   "source": [
    "df['banned_words_ratio'] = df['n_banned_words'] / df['n_words'] # Using highly optimized Numpy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T09:20:30.938550Z",
     "start_time": "2020-02-04T09:20:30.386745Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    words_per_year = df[df['Style'] == style].groupby('Annee')['banned_words_ratio'].mean()\n",
    "    sns.lineplot(words_per_year.index, words_per_year.values, label=style)\n",
    "plt.title('Mean ratio of banned words')\n",
    "plt.ylabel('Mean ratio of banned words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph allows us to draw some conclusions:\n",
    "- **Rap style includes more banned words both proportionnaly and in general**.\n",
    "- Many genres seem to be reducing their use of banned words through the years.\n",
    "- The **pop music style uses really few banned words**. This might be because its target audience is wider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Similarity\n",
    "- Vocabulary style (t-sne)\n",
    " - Récupérer un embedding existant (https://code.google.com/archive/p/word2vec/, https://tfhub.dev/google/nnlm-en-dim128/2, https://tfhub.dev/google/elmo/3) sur le vocabulaire, puis l'appliquer sur les paroles des musiques\n",
    "  - Appliquer un tSNE sur les paroles post embedding => noms de musique, couleur = style\n",
    "\n",
    "- Sentiment score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
