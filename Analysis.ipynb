{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Analysis\n",
    "We scrapped the AZLyrics website in the previous Jupyter Notebook, and are now going to use the collected data for Lyrics analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:21.533390Z",
     "start_time": "2020-02-05T14:46:16.392503Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import jellyfish\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:21.638242Z",
     "start_time": "2020-02-05T14:46:21.533390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import and merge the csv files generated during the data collection\n",
    "df= pd.DataFrame()\n",
    "for i in range(6):\n",
    "    try: # if file exists\n",
    "        df= pd.concat([df, pd.read_csv(\"data/0{}_dataset_lyrics.csv\".format(i+1))])\n",
    "    except:\n",
    "        #au cas ou , si mon fichier n'existe pas alors j'ajoute un df vide\n",
    "        df= pd.concat([df, pd.DataFrame()])\n",
    "        \n",
    "df= df.rename({\"test\":\"Lyrics\"}, axis=1)\n",
    "df.drop(columns='Url', inplace=True)\n",
    "df.loc[df['Style'] == 'Rock\\\\Alternatif', 'Style'] = 'Rock/Alternatif' # Fixing a small issue with the dataset\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:21.649147Z",
     "start_time": "2020-02-05T14:46:21.640172Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:21.671089Z",
     "start_time": "2020-02-05T14:46:21.652140Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have successfully scrapped the lyrics of over 1 thousand songs. We also extracted interesting meta data which are going to enhance our analysis:\n",
    "- Artist\n",
    "- Year\n",
    "- Album name\n",
    "- Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:22.316820Z",
     "start_time": "2020-02-05T14:46:21.674081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the lyrics into arrays of words\n",
    "df['words'] = df.apply(\n",
    "    lambda row: np.char.lower(np.array(re.findall(r'\\w+', row['Lyrics']))),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:22.467715Z",
     "start_time": "2020-02-05T14:46:22.316820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Erase the stop words\n",
    "stop_words = np.array(set(stopwords.words('english')))\n",
    "df['words'] = df['words'].apply(\n",
    "    lambda word_array: word_array[np.logical_not(np.isin(word_array, stop_words))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:22.487654Z",
     "start_time": "2020-02-05T14:46:22.469699Z"
    }
   },
   "outputs": [],
   "source": [
    "df['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Lyrics length\n",
    "The first interesting insight that we can get from our dataset is the number of words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:22.501615Z",
     "start_time": "2020-02-05T14:46:22.492639Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_words'] = df['words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:22.958134Z",
     "start_time": "2020-02-05T14:46:22.505603Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    words_per_year = df[df['Style'] == style].groupby('Annee')['n_words'].mean()\n",
    "    sns.lineplot(words_per_year.index, words_per_year.values, label=style)\n",
    "plt.title('Various styles mean number of words')\n",
    "plt.ylabel('Mean number of words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:24.241327Z",
     "start_time": "2020-02-05T14:46:22.961107Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(16, 12))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    for artist in df.loc[df['Style'] == style, 'Artiste'].unique():\n",
    "        words_per_year = df[df['Artiste'] == artist].groupby('Annee')['n_words'].mean()\n",
    "        sns.lineplot(words_per_year.index, words_per_year.values, label=artist, ax=ax[i])\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_xlabel('')\n",
    "plt.suptitle('Mean number of words per artist and per style')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of our music style have a homogeneous number of words around 300, except for the rap genre that is near double, around 600 words per music!\n",
    "\n",
    "Thus, **rap music generally includes more words per song than other music styles**.\n",
    "\n",
    "Nevertheless, **there is more heterogeneity in the rap style than in other styles**. For example, Eminem produces music with way more lyrics than Post Malone, even though they are both considered rappers.\n",
    "\n",
    "We should now have a look at the diversity of these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Unique words\n",
    "Now that we know the sizes of lyrics, let's have a look at their diversity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:24.456889Z",
     "start_time": "2020-02-05T14:46:24.243294Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_unique_words'] = df['words'].apply(\n",
    "    lambda word_list: len(set(word_list))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:24.867809Z",
     "start_time": "2020-02-05T14:46:24.458902Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    unique_words_per_year = df[df['Style'] == style].groupby('Annee')['n_unique_words'].mean()\n",
    "    sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=style)\n",
    "plt.title('Various styles mean number of unique words')\n",
    "plt.ylabel('Mean number of words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:26.123765Z",
     "start_time": "2020-02-05T14:46:24.869803Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(16, 12))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    for artist in df.loc[df['Style'] == style, 'Artiste'].unique():\n",
    "        unique_words_per_year = df[df['Artiste'] == artist].groupby('Annee')['n_unique_words'].mean()\n",
    "        sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=artist, ax=ax[i])\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_xlabel('')\n",
    "plt.suptitle('Mean number of unique words per artist and per style')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that, once again, the rap style uses more diverse words than the other music styles. This is no surprise, since it also uses more words (even non-unique) per songs, so we are going to look at the uniqueness ratio of the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:26.147706Z",
     "start_time": "2020-02-05T14:46:26.126759Z"
    }
   },
   "outputs": [],
   "source": [
    "df['unique_words_ratio'] = df['n_unique_words'] / df['n_words'] # Using highly optimized Numpy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:26.585530Z",
     "start_time": "2020-02-05T14:46:26.152689Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    unique_words_per_year = df[df['Style'] == style].groupby('Annee')['unique_words_ratio'].mean()\n",
    "    sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=style)\n",
    "plt.title('Various styles mean ratio of unique words')\n",
    "plt.ylabel('Mean number of words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is interesting! We can see two insights here:\n",
    "- Even though rap produces a higher quantity of lyrics, **the ratio of unique words is not largely higher than other genres**.\n",
    "- There are visible trends in the ratio of unique words:\n",
    " - **Metal diversity of vocabulary decline sinced 1995**.\n",
    " - **Pop diversity of vocabulary is rapidly increasing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:27.918026Z",
     "start_time": "2020-02-05T14:46:26.587526Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(16, 12))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    for artist in df.loc[df['Style'] == style, 'Artiste'].unique():\n",
    "        unique_words_per_year = df[df['Artiste'] == artist].groupby('Annee')['unique_words_ratio'].mean()\n",
    "        sns.lineplot(unique_words_per_year.index, unique_words_per_year.values, label=artist, ax=ax[i])\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_xlabel('')\n",
    "plt.suptitle('Mean ratio of unique words per artist and per style')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the rapid increase of ratio of unique words comes from Billie Eilish's work. Is this a general trend in the pop genre, or is this a specificity of our dataset? We would need more data to confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Which words?\n",
    "Now that we have analyzed the quantities of words in the various lyrics of our dataset, we are going to have a qualitative approach, and understand which words are being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Most commonly used words\n",
    "First, we are going to have a look at the most commonly used words. Nevertheless, we want to consider the various declinations of a word into one single word, not many. We are thus going to stem our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:33.842139Z",
     "start_time": "2020-02-05T14:46:27.921016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stemming the words before counting\n",
    "stemmer = EnglishStemmer()\n",
    "df['stem_words'] = df['words'].apply(\n",
    "    lambda word_list: [stemmer.stem(word) for word in word_list] #TODO: optimize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:34.009104Z",
     "start_time": "2020-02-05T14:46:33.844073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Counting the used words\n",
    "all_words = pd.DataFrame({'word': [word for row in df['stem_words'] for word in row], 'count': 1})\n",
    "all_words = all_words.groupby('word').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:34.887912Z",
     "start_time": "2020-02-05T14:46:34.011661Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.stem(all_words.sort_values('count', ascending=False)[:50].index,\n",
    "        all_words.sort_values('count', ascending=False)[:50])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Most comonly used words')\n",
    "plt.ylabel('Number of use')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:38.849553Z",
     "start_time": "2020-02-05T14:46:34.887912Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(14,10))\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    # Counting the used words\n",
    "    all_words = pd.DataFrame({'word': [word for row in df.loc[df['Style']==style, 'stem_words'] for word in row], 'count': 1})\n",
    "    all_words = all_words.groupby('word').sum()\n",
    "    ax[i].stem(\n",
    "        all_words.sort_values('count', ascending=False)[:50].index,\n",
    "        all_words.sort_values('count', ascending=False)[:50]\n",
    "    )\n",
    "    ax[i].xaxis.set_tick_params(rotation=90)\n",
    "    ax[i].set_title(style)\n",
    "    ax[i].set_ylabel('Number of use')\n",
    "plt.suptitle('Most commonly used word per style')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Vulgarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to have a look at the vulgarity of the lyrics of our dataset.\n",
    "\n",
    "In order to do so, we are going to use a [ban word list](https://www.freewebheaders.com/bad-words-list-and-page-moderation-words-list-for-facebook/) which is used for facebook moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:38.864515Z",
     "start_time": "2020-02-05T14:46:38.852546Z"
    }
   },
   "outputs": [],
   "source": [
    "ban_words = pd.read_csv(\"data/ban_word.csv\", header= None, names= [\"nono\"])\n",
    "ban_words = ban_words.drop(index= 0).reset_index(drop= True)\n",
    "ban_words = np.array(ban_words.values)\n",
    "print(f'There are {len(ban_words)} banned words in our list.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:49.610004Z",
     "start_time": "2020-02-05T14:46:38.866619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the banned_words\n",
    "df['banned_words'] = df['words'].apply(\n",
    "    lambda word_array: word_array[np.isin(word_array, ban_words)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:49.624231Z",
     "start_time": "2020-02-05T14:46:49.615916Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_banned_words'] = df['banned_words'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:50.058465Z",
     "start_time": "2020-02-05T14:46:49.631211Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    words_per_year = df[df['Style'] == style].groupby('Annee')['n_banned_words'].mean()\n",
    "    sns.lineplot(words_per_year.index, words_per_year.values, label=style)\n",
    "plt.title('Mean number of banned words')\n",
    "plt.ylabel('Mean number of banned words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the rap music style is, once again, the highest one on the charts, but this is caused by the cheer number of word in their songs. Let's have a look at the ratio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:50.069401Z",
     "start_time": "2020-02-05T14:46:50.061421Z"
    }
   },
   "outputs": [],
   "source": [
    "df['banned_words_ratio'] = df['n_banned_words'] / df['n_words'] # Using highly optimized Numpy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:50.500130Z",
     "start_time": "2020-02-05T14:46:50.073391Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for style in df['Style'].unique():\n",
    "    words_per_year = df[df['Style'] == style].groupby('Annee')['banned_words_ratio'].mean()\n",
    "    sns.lineplot(words_per_year.index, words_per_year.values, label=style)\n",
    "plt.title('Mean ratio of banned words')\n",
    "plt.ylabel('Mean ratio of banned words')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph allows us to draw some conclusions:\n",
    "- **Rap style includes more banned words both proportionnaly and in general**.\n",
    "- Many genres seem to be reducing their use of banned words through the years.\n",
    "- The **pop music style uses really few banned words**. This might be because its target audience is wider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the similarity of lyrics within styles and between styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:50.507086Z",
     "start_time": "2020-02-05T14:46:50.503099Z"
    }
   },
   "outputs": [],
   "source": [
    "# lyrics_matrix = np.stack(\n",
    "#     [\n",
    "#         np.tril(\n",
    "#             np.repeat(\n",
    "#                 np.reshape(df['Lyrics'].values, (len(df), 1)),\n",
    "#                 len(df),\n",
    "#                 1\n",
    "#             )\n",
    "#         ),\n",
    "#         np.tril(\n",
    "#             np.repeat(\n",
    "#                 np.reshape(df['Lyrics'].values, (1, len(df))),\n",
    "#                 len(df),\n",
    "#                 0\n",
    "#             )\n",
    "#         )\n",
    "#     ],\n",
    "#     axis=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:50.516094Z",
     "start_time": "2020-02-05T14:46:50.510080Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.apply_along_axis(\n",
    "#     lambda couple: jellyfish.levenshtein_distance(str(couple[0]), str(couple[1])),\n",
    "#     axis=-1,\n",
    "#     arr=lyrics_matrix,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vocabulary style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to try and visualize the vocabulary used by the various lyrics we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:46:53.659945Z",
     "start_time": "2020-02-05T14:46:50.519053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the nnlm-en-dim128 TensorFlow embedding model available at\n",
    "# https://tfhub.dev/google/nnlm-en-dim128/2\n",
    "# The model is > 450MB, download might take a long time (25 minutes).\n",
    "# Nevertheless, tensorflow hub caches the model, so the next uses are way faster (2 seconds).\n",
    "embed = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\", input_shape=[], dtype=tf.string)\n",
    "\n",
    "# Embedding of the Lyrics to a 128 dimensions dense tf.Tensor\n",
    "embeddings = embed(df['Lyrics'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:01.838083Z",
     "start_time": "2020-02-05T14:46:53.659945Z"
    }
   },
   "outputs": [],
   "source": [
    "# t-SNE 2 dimensional projection of the embeddings\n",
    "projections = TSNE(n_components=2).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:02.350008Z",
     "start_time": "2020-02-05T14:47:01.838083Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=projections[:, 0], y=projections[:, 1], hue=df['Style'].values)\n",
    "plt.title('Lyrics projection by style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize and interpret the projections of our lyrics.\n",
    "\n",
    "As we can see, rap lyrics are definitely different than metal lyrics, it is easily visible. On the other hand, metal and rock/alternatif lyrics seem really similar. Finally, pop lyrics seem really diversified and do not visibly differ with other music styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:03.623408Z",
     "start_time": "2020-02-05T14:47:02.352998Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12), sharex='all', sharey='all')\n",
    "for (i, style) in enumerate(df['Style'].unique()):\n",
    "    mask = df['Style'] == style\n",
    "    sns.scatterplot(\n",
    "        x=projections[mask, 0],\n",
    "        y=projections[mask, 1],\n",
    "        hue=df.loc[mask, 'Artiste'].values,\n",
    "        ax=ax.flat[i]\n",
    "    )\n",
    "    ax.flat[i].set_title(style)\n",
    "plt.suptitle('Lyrics projection by artist')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dig deeper and visualize artist by artist.\n",
    "\n",
    "One good application would be to use this dataset to predict music style / artist when given a lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Predict music style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to construct a model based on the embedder used earlier to try and predict the music style of our lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:04.383212Z",
     "start_time": "2020-02-05T14:47:03.625403Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embed)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:04.396178Z",
     "start_time": "2020-02-05T14:47:04.386202Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df['Lyrics'].values\n",
    "y = pd.get_dummies(df['Style']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:21.114316Z",
     "start_time": "2020-02-05T14:47:04.399166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:21.424489Z",
     "start_time": "2020-02-05T14:47:21.116309Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model which extends the nnlm model is capable, with a small dataset of only a thourand lyrics, of differencing several music styles.\n",
    "\n",
    "It achieves an accuracy of 79% and a ROC AUC of 0.94!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Predict Artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to try and predict the artist of a lyrics witht the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:39.805937Z",
     "start_time": "2020-02-05T14:47:21.427387Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(embed)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(len(df['Artiste'].unique()), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Prepare the dataset\n",
    "X = df['Lyrics'].values\n",
    "y = pd.get_dummies(df['Artiste']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:47:40.156483Z",
     "start_time": "2020-02-05T14:47:39.807932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the results\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model achieves good results here too, even though less good than on the music style prediction, which is probably easier to modelise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next step to improve our models is to collect more data and restart the training.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
